{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1567181,"sourceType":"datasetVersion","datasetId":925857}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport numpy as np\nimport os\nimport time\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(image_path):\n    img = load_img(image_path, target_size=(256, 256))\n    img = img_to_array(img)\n    img = (img / 127.5) - 1  # Normalize image to [-1, 1]\n    return img\n\ndef load_dataset(dataset_path):\n    images = []\n    for filename in os.listdir(dataset_path):\n        img = load_image(os.path.join(dataset_path, filename))\n        images.append(img)\n    return np.array(images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def residual_block(x, filters, size=3, stride=1):\n    \n    res = layers.Conv2D(filters, size, strides=stride, padding=\"same\")(x)\n    res = layers.BatchNormalization()(res)\n    res = layers.LeakyReLU(0.2)(res)\n\n    res = layers.Conv2D(filters, size, strides=stride, padding=\"same\")(res)\n    res = layers.BatchNormalization()(res)\n    return layers.add([x, res])\n\ndef build_generator(input_shape=(256, 256, 3)):\n    inputs = layers.Input(shape=input_shape)\n\n    x = layers.Conv2D(64, 7, strides=1, padding=\"same\")(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    # Down-sampling layers\n    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2D(256, 3, strides=2, padding=\"same\")(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    # Residual Blocks\n    for _ in range(6):\n        x = residual_block(x, 256)\n\n    # Upsampling layers\n    x = layers.Conv2DTranspose(128, 3, strides=2, padding=\"same\")(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\")(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.BatchNormalization()(x)\n\n    outputs = layers.Conv2D(3, 7, strides=1, padding=\"same\", activation='tanh')(x)\n\n    model = Model(inputs, outputs)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_discriminator():\n    inputs = layers.Input(shape=[256, 256, 3])\n\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n    x = layers.LeakyReLU(0.2)(x)\n    \n    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(512, 4, strides=2, padding='same')(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n\n    model = Model(inputs, x)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cycle_loss(real_image, cycled_image):\n    return tf.reduce_mean(tf.abs(real_image - cycled_image))\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    disc_real_output = tf.reshape(disc_real_output, [-1])  # Flattening the output\n    disc_generated_output = tf.reshape(disc_generated_output, [-1])  # Flattening the output\n    \n    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_real_output), logits=disc_real_output))\n    generated_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(disc_generated_output), logits=disc_generated_output))\n    total_disc_loss = (real_loss + generated_loss) * 0.5\n    return total_disc_loss\n\ndef generator_loss(disc_generated_output, cycled_image, real_image, lambda_cycle=10.0):\n    gan_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_generated_output), logits=disc_generated_output))\n    cycle_loss_value = cycle_loss(real_image, cycled_image)\n    total_gen_loss = gan_loss + (lambda_cycle * cycle_loss_value)\n    return total_gen_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator_g = build_generator()\ngenerator_f = build_generator()\ndiscriminator_x = build_discriminator()\ndiscriminator_y = build_discriminator()\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, decay_steps=100000, decay_rate=0.96, staircase=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n    with tf.GradientTape() as tape:\n\n        real_x = tf.expand_dims(real_x, axis=0)  # Shape becomes (1, 256, 256, 3)\n        real_y = tf.expand_dims(real_y, axis=0)  # Shape becomes (1, 256, 256, 3)\n        \n        # Generate fake images\n        fake_y = generator_g(real_x)  # Generate winter image from summer\n        fake_x = generator_f(real_y)  # Generate summer image from winter\n        \n        # Cycle consistency loss\n        cycled_x = generator_f(fake_y)\n        cycled_y = generator_g(fake_x)\n        \n        # Identity loss (optional, if included in CycleGAN)\n        identity_loss = tf.reduce_mean(tf.abs(real_y - fake_y)) + tf.reduce_mean(tf.abs(real_x - fake_x))\n        \n        # Define the discriminator losses, generator losses, and cycle consistency losses\n        disc_x_loss = discriminator_x(real_x, fake_x)\n        disc_y_loss = discriminator_y(real_y, fake_y)\n        \n        gen_g_loss = generator_loss(disc_y_loss, cycled_y, real_x)  # Generator loss for generating fake winter images\n        gen_f_loss = generator_loss(disc_x_loss, cycled_x, real_y)  # Generator loss for generating fake sumer images\n        \n        # Total loss (including identity loss and cycle consistency loss if used)\n        total_gen_loss = gen_g_loss + gen_f_loss + identity_loss\n        \n    return gen_g_loss, gen_f_loss, disc_x_loss, disc_y_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images(real_x, fake_y, real_y , fake_x, num_images):\n    plt.figure(figsize=(10, 10))\n\n    for i in range(num_images):\n        # Plot real summer images\n        plt.subplot(num_images, 2, 2 * i + 1)\n        plt.imshow((real_x[i] + 1) / 2)  # Rescale to [0, 1]\n        plt.title(f\"Real Summer {i+1}\")\n        plt.axis('off')\n\n        # Plot fake winter images\n        plt.subplot(num_images, 2, 2 * i + 2)\n        plt.imshow((fake_y[i] + 1) / 2)  # Rescale to [0, 1]\n        plt.title(f\"Fake Winter {i+1}\")\n        plt.axis('off')\n        \n    for i in range(num_images):\n        # Plot real winter images\n        plt.subplot(num_images, 2, 2 * i + 1)\n        plt.imshow((real_y[i] + 1) / 2)  # Rescale to [0, 1]\n        plt.title(f\"Real Winter {i+1}\")\n        plt.axis('off')\n\n        # Plot fake summer images\n        plt.subplot(num_images, 2, 2 * i + 2)\n        plt.imshow((fake_x[i] + 1) / 2)  # Rescale to [0, 1]\n        plt.title(f\"Fake Summer {i+1}\")\n        plt.axis('off')\n        \n    plt.tight_layout()\n    plt.show()\n\ndef train(dataset_x, dataset_y, num_images=10):\n\n        for real_x, real_y in zip(dataset_x, dataset_y):\n            # Train the model with real summer and winer images\n            gen_g_loss, gen_f_loss, disc_x_loss, disc_y_loss = train_step(real_x, real_y)\n\n            selected_indices = np.random.randint(len(dataset_x), size=num_images)\n            selected_real_x = dataset_x[selected_indices]  # 10 randomly selected summer images\n\n            fake_y = generator_g(selected_real_x)  # Generate fake winter images\n\n            selected_indices = np.random.randint(len(dataset_y), size=num_images)\n            selected_real_y = dataset_y[selected_indices]  # 10 randomly selected winter images\n\n            fake_x = generator_f(selected_real_y)  # Generate fake summer images\n\n            # Display real vs fake images\n            display_images(selected_real_x, fake_y, selected_real_y, fake_x, num_images)\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\nsummer_dataset = load_dataset('/kaggle/input/summer2winter-yosemite/trainA') \nwinter_dataset = load_dataset('/kaggle/input/summer2winter-yosemite/trainB')\n\n# Train the model\ntrain(summer_dataset, winter_dataset, num_images=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}